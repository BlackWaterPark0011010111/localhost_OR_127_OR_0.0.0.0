Что такое Rest API (http)? Soap? GraphQL? Websockets? RPC (gRPC, tRPC). Клиент - сервер. Вся теория

Итак, начнём с плана на урок. Начнём мы, конечно, с введения — поговорим про клиент-серверное взаимодействие в целом, про основу всего этого — HTTP-протокол, архитектурные стили и так далее. Дальше будем рассматривать уже конкретные сценарии: поговорим, конечно же, про REST API, про SOAP, про GraphQL, про WebSockets, про RPC на примере gRPC и tRPC.
Сразу добавлю, что прямо в какие-то дебри мы здесь углубляться не будем — здесь будет ролик больше обзорный про
разные технологии. Но если вам нужно про какой-то отдельный вариант (например, про GraphQL, или про WebSockets, или
про RPC) записать какой-то отдельный подробный, углублённый ролик — обязательно пишите об этом в комментарии, и я
над этим подумаю.
Итак, двигаемся дальше. Более дотошные зрители наверняка сейчас зададут вопрос: почему я HTTP-протокол объединил с
архитектурным стилем (REST, SOAP) или же, например, с GraphQL — с языком запросов? Ведь по сути это вообще разное.
И на самом деле они отчасти будут правы. Однако всё, что вы видите на слайде, объединяет то, что это
клиент-серверная архитектура. Так или иначе, это всё определяет способ общения клиента и сервера, и именно об этих
способах общения мы сегодня и будем говорить.
Итак, начнём с самых-самых основ — поговорим в целом про клиент-серверную архитектуру. В такой архитектуре сервер
выступает источником, скажем так, вычислений, хранения данных, какой-то бизнес-логики, определяет API, о котором мы
дальше тоже будем говорить. А клиент в этой всей архитектуре с помощью каких-то определённых действий может с
сервером общаться: может попросить какие-то данные у него, и сервер ему эти данные вернёт.
И самое главное, что всё это работает в формате «запрос-ответ». То есть клиент данные попросил — сервер эти данные
вернул. И как вы понимаете, речь идёт не только про данные — мы также можем серверу сказать выполнить какое-то
действие: например, зарегистрировать нового пользователя, провести оплату, добавить товар в корзину. И
соответственно сервер уже определяет, может он выполнить это действие или не может, и в зависимости от этого он
возвращает нам разный ответ.
И как я уже говорил, сервер выступает таким единым источником вычислений, бизнес-логики. И вот начинающие (особенно
фронтенд-разработчики) ошибочно думают, что клиент — это всегда какой-то браузер. Но на самом деле это может быть и
мобильное приложение, и другой сервер, который обращается к нашему серверу. То есть сама аббревиатура
«клиент-сервер» подразумевает, что сервер — это поставщик каких-то данных, услуг, логики, а клиент — это
потребитель этих данных. То есть опять же клиентом может выступать не обязательно браузер — это может быть и
десктопное приложение, и другой сервер, который обращается к нашему серверу за какими-то данными или просит
выполнить какое-то действие.
При этом для клиентов в большинстве случаев сервер — это такое единое, цельное, как я уже сказал, какое-то
монолитное приложение, источник бизнес-данных. Но при этом сам сервер под капотом может быть большим,
распределённым, состоящим из нескольких микросервисов. Но при этом клиента это особо волновать не должно.
Ну и в общем подытожим: клиент — это у нас потребитель, а сервер — поставщик. С клиент-серверной архитектурой,
думаю, понятно.
Теперь поговорим про основу всего — про то, на чём в принципе основан весь веб. Это HTTP.
HTTP — это протокол, гипертекстовый транспортный протокол, который находится на самом верхнем уровне модели OSI /
модели TCP/IP — это прикладной уровень, или уровень приложений, как его по-другому называют. Именно с помощью HTTP
общаются большинство приложений в интернете.
Первоначальная идея HTTP — это обмен гипертекстовыми документами (то есть нашими HTML-ками, которые мы так хорошо
все знаем). Но сейчас с помощью HTTP можно по сети передавать практически любые данные: текстовые, файлы, HTML,
XML, JSON — в общем, практически любой формат.
Теперь поговорим про структуру самого HTTP и посмотрим на пример запроса. Самый классический, базовый запрос
выглядит примерно вот так. Сходу для тех, кто никогда этого раньше не видел, выглядит достаточно страшно, поэтому
давайте разобьём HTTP-запрос на составные части и посмотрим на структуру.
Первая строчка HTTP-запроса — это стартовая строка, и состоит она из трёх частей:
Метод
URL
Версия HTTP
(Здесь я извиняюсь — при монтаже сделал ошибку, заметил уже достаточно поздно, всё переделывать не хочется. URL
здесь — это /auth/login, а версия — это HTTP/1.1. То есть в серединку вот в этот URL попал HTTP ошибочно.)
Метод — это, грубо говоря, семантика запроса: что мы хотим сделать — создать какой-то ресурс, обновить его или же
получить. В данном случае POST говорит нам о том, что мы хотим какой-то ресурс создать или выполнить какое-то
действие. Про методы мы более подробно чуть позже поговорим.
URL — это то, куда мы отправляем запрос. В данном случае мы отправляем его на /auth/login — тем самым сервер
понимает, какое именно действие мы хотим сделать.
Ну и далее указывается версия HTTP — тут, думаю, всё понятно. Со стартовой строкой разобрались.
Теперь посмотрим на следующую часть структуры HTTP — это заголовки (так называемые headers). Заголовки несут
достаточно важную информацию. В них есть часть заголовков обязательных и часть заголовков необязательных. В том
числе можно составлять какие-то свои, кастомные заголовки.
В заголовках указывается:
информация о хосте, с которого был отправлен запрос,
информация о браузере или о типе устройства, с которого отправляется запрос,
тип контента (например, text/html или какой-то другой тип),
также могут отправляться различные авторизационные заголовки с токенами,
также с помощью заголовков обеспечивается безопасность взаимодействия различных источников сети (так называемый
CORS — заголовки, которые отвечают за безопасность общения разных источников, которые расположены на разных
доменах).
И третья часть любого HTTP-запроса — это тело сообщения. В нём клиент отправляет серверу какие-то данные, которые
ему необходимы.
Давайте этот запрос расшифруем: в сообщении мы отправляем логин пользователя и пароль для того, чтобы сервер мог
этого пользователя распознать.
То есть, давайте ещё раз рассмотрим наглядно: клиент отправляет запрос, который выглядит как раз таким образом, как
мы сейчас рассмотрели. Сервер должен вернуть ему ответ.
Ответ немного отличается от запроса и выглядит он примерно таким образом. Из отличий сразу бросается в глаза, что
стартовая строка выглядит немного по-другому. Всё дело в том, что в ответе стартовая строка (она называется «строка
статуса») содержит:
версию HTTP,
так называемый статус-код, который определяет, успешно был выполнен запрос или нет.
Также в ответе можно заметить такие же headers (заголовки), но они могут отличаться от тех, которые были в запросе.
То есть есть заголовки запроса и есть заголовки ответа. И точно так же есть тело сообщения, в котором сервер
возвращает клиенту какие-то данные.
Теперь поговорим про важную часть HTTP — это методы (та самая часть стартовой строки, о которой мы говорили ранее).
Методов достаточно большое количество, но мы сейчас разберём только основные, которыми чаще всего оперирует
разработчик.
Итак, это методы:
GET
POST
PUT
PATCH
DELETE
В зависимости от того, какой метод указал клиент, сервер будет обрабатывать запрос по-разному. Семантически каждый
из них выполняет какое-то определённое действие:
GET предназначен для получения данных (например, запросить список товаров, запросить список пользователей,
запросить какую-то информацию расширенную по конкретному товару).
POST чаще всего предназначен либо для создания ресурса, либо для выполнения какого-то действия (как, например, 
auth/login, который мы рассматривали ранее).
PUT и PATCH предназначены для обновления ресурса, но семантически они тоже разные: PUT обновляет ресурс целиком, а
PATCH как правило обновляет только часть ресурса.
DELETE — думаю, всё понятно, он предназначен для удаления какого-то ресурса (удалить пользователя, удалить товар).
Со стартовой строкой HTTP-запроса разобрались. Теперь давайте разберёмся со строкой статуса, которую возвращает
сервер уже в виде ответа. И здесь самой важной частью для нас является статус-код, который сообщает нам о том, как
именно был обработан запрос.
Существует пять групп этих статус-кодов:
Информационные (1xx) — они всегда начинаются с единички (например, 100, 101, 102 и так далее).
Успешные (2xx) — статус-коды, которые начинаются с двойки, обозначают, что запрос выполнился успешно (200-й
статус-код думаю так или иначе встречал абсолютно каждый).
Редиректы (3xx) — статус-коды, которые начинаются с тройки, обычно означают какой-то редирект, перенаправление
(например, мы успешно залогинились — нас перенаправляет на другую страницу).
Ошибки клиента (4xx) — статус-коды, которые начинаются с четвёрки, обозначают, что запрос был обработан с ошибкой
(например, клиент передал неправильные данные). Думаю, 404 (легендарный статус-код, который говорит о том, что
какой-то ресурс не найден) видели также все.
Ошибки сервера (5xx) — статус-коды, которые начинаются с пятёрки — это ошибки сервера (например, клиент передал
данные все корректные, но на сервере написали неправильный код, там что-то сломалось, и сервер соответственно
возвращает 500-й статус-код). Обычно это называют «сервер пятисот».
Опять же, статус-коды — это про семантику. То есть ничего не мешает нам, например, на сервере выполнить какое-то
действие с ошибкой, но вернуть 200-й статус-код. Но так обычно никто не делает.
То же самое и с методами — ничего не мешает нам сделать так, чтобы GET-запрос у нас создавал какой-то ресурс, а не
возвращал какие-то данные. Но опять же так никто не делает.
Ну давайте рассмотрим ещё примеры каких-то статус-кодов:
тот самый легендарный 404 Not Found — когда мы не нашли ресурс,
400 Bad Request — статус-код чаще всего говорит, что запрос был некорректным (то есть опять же это всё про
семантику — ничего нам не мешает всегда ставить 400-й статус-код на все клиентские ошибки, но мы их подразделяем),
201 Created — статус-код говорит об успехе, о том, что ресурс был успешно создан,
500 Internal Server Error — о том, что это внутренняя ошибка сервера.
Итого мы уяснили, что семантика здесь очень важна, и хотелось бы иметь определённый набор правил, какой-то
архитектурный стиль, который говорит нам о том, как делать правильно.
API - application programming interface
Как неправильно? И для этого существует REST - архитектурный стиль, проверенный временем, который сообщает нам о
том, как наиболее эффективно общаться клиенту и серверу по HTTP.
Перед тем как говорить про сам REST, давайте разберём вообще понятие API. Расшифровывается оно как Application
Programming Interface. То есть из названия уже понятно, что это некоторый интерфейс, который описывает то, как с
программой стоит общаться. И это интерфейс именно программный.
Как альтернативу можно привести графический интерфейс - то есть мы открываем приложение, нажимаем на какие-то
кнопочки, взаимодействуем с графическим интерфейсом, тем самым мы сообщаем программе, какие действия мы хотим
выполнить. А API - это именно программный интерфейс, который предоставляет способ взаимодействия с этой программой.
Предлагаю сразу рассмотреть на конкретном примере. Допустим, у нас есть программа, обратившись к которой мы можем
сделать следующие действия:
Получить актуальный курс валют
Переконвертировать одну валюту в другую
Получить прогноз погоды на неделю
Допустим, мы хотим получить прогноз погоды на неделю. Для этого сервер нам должен предоставить какой-то интерфейс,
с помощью которого мы можем это сделать. Из описания этого интерфейса мы понимаем, что мы должны отправить
GET-запрос по определённому URL, в query-параметрах указать дату, на которую нам нужен прогноз, и город, для
которого мы этот прогноз хотим получить. А сервер в свою очередь в качестве тела ответа вернёт нам в виде массива,
состоящего из объектов, в котором есть время, температура, будет ли дождь и так далее - всю необходимую для нас
информацию.
Исходя из того, что мы видим, мы понимаем, что отправив запрос с соответствующими параметрами, мы получим
соответствующий ответ. Это и есть API - то, как мы взаимодействуем с сервером. Хорошая API всегда предоставляет
хорошую документацию, где подробно расписан список методов (так называемых endpoint'ов), которые мы можем
использовать для выполнения определённых действий. Также должно быть описание, что ожидается на вход для этого
endpoint'а и что ожидается на выходе.
REST API: архитектурный стиль
Итак, с вводной частью мы разобрались, теперь переходим непосредственно к REST. Многие ошибочно называют REST
протоколом, но это далеко не протокол. Протоколом в данном случае выступает HTTP. REST - это архитектурный стиль,
набор правил, который описывает, как использовать HTTP и строить свою API так, чтобы ей было удобно пользоваться,
чтобы она выдерживала нагрузки, легко масштабировалась и так далее. То есть чтобы мы извлекали все плюсы.
Сейчас мы по основным правилам пройдёмся, разберём их, посмотрим на концепции, которые заложены в основу REST.
Первая концепция говорит нам о том, что модель взаимодействия с API - это клиент-сервер. Не зря я именно с этого
начал. В данном видео API у нас представлена в виде сервера, а потребителями (клиентами) могут быть как десктопные,
браузерные, мобильные приложения, так и другие сервера, которые по какому-то контракту общаются с нашим сервером.
Следующая концепция - это многоуровневость или многослойность системы. То есть по REST система может иметь n-ное
количество слоёв, причём с точки зрения клиента это вообще никакой роли не играет. С точки зрения клиента эта вся
система она как бы цельная, единая, а внутри там может быть сколько угодно много слоёв с каким-нибудь
балансированием, распределением, проксированием, с какой-нибудь микросервисной архитектурой.
Третья концепция говорит нам о том, что сервер не должен обладать каким-либо состоянием (stateless). Что это
значит? Давайте рассмотрим на примере: клиент отправляет запрос, сервер возвращает ему какой-то ответ, и при этом
никакого промежуточного состояния не запоминается (именно с точки зрения сервера). Какие-то данные могут вноситься
в БД, в кэш ещё куда-то - это мы не рассматриваем. То есть подразумевается, что при каждом следующем запросе клиент
и сервер общаются как будто бы в первый раз.
Для того чтобы сервер идентифицировал клиента, для того чтобы сервер понял, что ему необходимо сделать, какие
данные вернуть, какую операцию произвести, клиент должен отправлять всю необходимую, полную информацию для того,
чтобы сервер смог этот запрос обработать и выполнить. Это в принципе одна из самых важных концепций. Для начинающих
могут быть какие-то трудности в понимании, однако если воспринимать вот эту концепцию отсутствия состояния как то,
что клиент и сервер каждый раз общаются как в первый раз, и их общение должно обладать полной информацией для того,
чтобы сервер смог обработать запрос, то в принципе всё должно быть понятно.
Четвёртая концепция - это единообразный (унифицированный) интерфейс, которым API обладает. Здесь довольно сложное
описание того, что это значит. Постараюсь на конкретном примере максимально просто объяснить.
Допустим, у нас интернет-магазин. Над любыми сущностями, например над товарами, мы можем выполнять так называемые
CRUD-операции (Create, Read, Update, Delete). То есть:
Добавить товар
Удалить товар
Обновить информацию о товаре
Получить список товаров (массив)
По ID товара получить информацию именно по конкретному товару
Как с такими операциями нам взаимодействовать? Ранее мы рассматривали HTTP-методы POST, DELETE, PUT, PATCH и GET.
Каждый из этих методов обладает определённой семантикой:
POST - создание
DELETE - удаление
PUT/PATCH - обновление
GET - получение
То есть для каждой CRUD-операции мы используем семантически правильный HTTP-метод. Далее, для каждой сущности у нас
есть определённый URL, по которому мы с ней взаимодействуем. В данном случае для товаров это URL /products
(обратите внимание - во множественном числе, так по REST более правильно называть эти URL для endpoint'ов).
Если мы хотим получить информацию по конкретному товару, то у нас в конце добавляется ещё ID: /products/{id}. Также
там может быть slug или ещё какой-то атрибут, по которому мы делаем поиск. Вот именно такая работа со всеми
endpoint'ами и есть тот самый единообразный, унифицированный интерфейс.
Сюда же можно ещё добавить формат взаимодействия (JSON, XML), заголовки, которые требуются для авторизации
пользователя (какой-нибудь токен). Не может быть такого, что у вас в одном endpoint'е он называется
Authorization-Token, а в другом X-Token. Всё должно быть единообразно, в одном стиле, в соответствии с правилами,
которые диктует REST.
Хочется также дополнительно акцентировать внимание на том, что запрос должен содержать всю необходимую информацию
для его выполнения. Мы не можем просто взять и сказать серверу "создай товар", просто отправив POST-запрос. Мы
должны предоставить всё необходимое, что требуется для сервера, чтобы он этот товар смог создать. То есть помимо
самого POST-запроса мы должны также указать информацию о товаре в теле запроса, указать соответствующие заголовки,
метод, указать соответствующий URL, и только тогда, когда мы передали всю необходимую информацию, сервер сможет
этот запрос обработать.
Также очень важный момент, про который я уже тоже говорил - это семантика. То есть, например, с помощью GET-запроса
мы должны получать данные, а изменять, удалять или создавать мы не должны. Хотя в теории, чисто технически мы можем
так сделать - ничего не мешает нам вообще использовать на всё GET-запросы и любую операцию делать через них. HTTP
это позволяет. Но семантика REST - это как раз про семантику, про наиболее эффективное использование HTTP. Мы
нарушим её, если будем использовать GET для изменения данных.
Если с GET и POST-запросами всё понятно (GET - получить ресурс, POST - создать ресурс), то с PUT всё немного
сложнее. Есть такое понятие как идемпотентность запросов. Причём какие-то запросы считаются идемпотентными, а
какие-то априори не могут быть таковыми.
Идемпотентность
Метод HTTP считается идемпотентным, если повторный запрос, сделанный несколько раз подряд, имеет один и тот же
эффект. То есть если мы пытаемся обновить какой-то ресурс и отправляем один и тот же запрос с одинаковыми данными
на обновление, то сервер должен выполнить этот запрос одинаково. Не должно быть такого, что первый запрос сделал
одни действия, второй запрос сделал другие действия - в итоге результат у нас непредсказуемый.
Если прямо зачитывать определение, то идемпотентность - это свойство, которое означает, что повторно идентичный
запрос, сделанный несколько раз подряд, имеет один и тот же эффект и не изменяет состояние сервера. Корректно
реализованные методы GET, PUT и DELETE идемпотентны. Но вот POST, например, идемпотентным по своей природе быть не
может.
Давайте на наглядном примере разберём, почему так. Если мы отправим POST-запрос с одинаковыми данными несколько раз
подряд, сервер создаст у нас несколько разных ресурсов. То есть результат здесь уже отчасти предсказуемый, но не
совсем - ресурсы всё равно создаются разные, эффект происходит не один и тот же. Именно поэтому POST-запрос не
идемпотентен.
Давайте посмотрим теперь на примере PUT-запроса. Если мы меняем возраст у какого-то ресурса (например,
пользователя) с 10 на 20, а потом ещё трижды отправим такой же PUT-запрос с попыткой заменить возраст на 20, у нас
ничего не произойдёт. Сервер не создаст нового ресурса, не удалит его - он просто обновит тот же самый атрибут у
какой-то сущности.
Пятая концепция - это кэширование. Причём кэширование может быть осуществлено средствами HTTP (за счёт проставления
определённых заголовков), так и средствами какими-то сторонними, реализованными на сервере (использование
какого-нибудь Redis, Memcached и так далее).
Причём важный момент с точки зрения семантики: GET-запросы могут кэшироваться, а PUT и DELETE - не могут.
Рассмотрим на примере, что вообще кэширование подразумевает.
Представим, что у нас есть сервер и миллион клиентов одновременно (или не одновременно, с каким-то промежутком
времени) запрашивают у нас список валют с помощью GET-запроса. Согласитесь, что список валют меняется крайне редко,
и каждый раз нам полноценно отправлять запрос, полноценно ходить в базу данных, полноценно доставать этот список
валют, чтобы вернуть его клиенту - смысла особого не имеет.
Поэтому мы можем иметь кэш. Этот кэш может быть реализован с помощью HTTP-заголовков, и результаты этого кэша будут
храниться, например, прямо в браузере, либо с помощью каких-то сторонних систем, которые реализованы на сервере
(например, Redis).

На примере HTTP-заголовков: мы указали, что хотим кэшировать результат, сохранили где-то его в браузере (нас это не
особо интересует, где это всё хранится - это подкапотные махинации). Теперь в следующий раз, когда наш клиент
запрашивает список валют, мы смотрим, что он у нас находится уже в кэше, и не отправляем запрос на сервер
повторный, а достаём из кэша. То есть это происходит намного быстрее, и как побочный эффект - снижает нагрузку на
сервер.Можете там сказать какую-то команду "Алиса, включи свет" и свет у вас включится, то есть своего рода такая
оптимизация вашего времени. Вот с кэшом в принципе то же самое - мы не ходим каждый раз в базу данных, не
отправляем каждый раз запрос, а достаём данные, которые редко меняются, из этого кэша.
Теперь поговорим про формат обмена данными между клиентом и сервером. Вообще по REST можно обмениваться практически
любыми данными, но в большинстве своём это JSON. Но также достаточно часто используется и XML. Сейчас на слайде вы
можете увидеть пример XML и JSON. В данном случае по наполнению, по самим данным они равны. Как я уже сказал, чаще
всего используют JSON, но XML однако тоже встречается, особенно в какой-то сфере финтеха, банковской сфере XML
достаточно распространён. Ещё позже в рамках курса поговорим про XML поподробнее.
Следующий важный момент, о котором стоит поговорить - это версионирование. Сразу рассмотрим на примере: представим
себе ситуацию, что у нас есть какой-то набор эндпоинтов, мы с ним работаем. У нас есть какие-то клиенты, которые
эти эндпоинты используют, и в какой-то момент нам необходимо внести какие-то правки в нашу API. Но эти правки не
обратно совместимы. То есть, например, мы хотим убрать вообще метод удаления пользователя или как-то изменить
формат общения. И если мы это сделаем прямо в текущем коде, в текущей версии нашей API, то у всех клиентов, которые
с нами работают, всё сломается.
Но я думаю, все прекрасно понимают, что все правки, которые вы вносите, должны быть обратно совместимыми. И
конечно, когда вы вносите вот такие вот правки, которые как-то меняют формат данных, надо менять именно версию. Те
пользователи, которые работают с первой версией API, они так и продолжают с ней работать, а новые пользователи или
пользователи, которые хотят сделать миграцию, начинают работать с новой версии - со второй, третьей, четвёртой,
пятой и так далее.
Следующий важный момент - это документация вашей API. Опять же, это не свойственно именно REST, это свойственно в
принципе любому API. Но у REST есть даже определённые спецификации, потому как стоит документировать и описывать
ваше API. И здесь стоит обратить внимание на OpenAPI. Это спецификация и Swagger. Давайте о них поговорим.
OpenAPI - это спецификация, которая позволяет вам задокументировать API: то, какие у вас есть эндпоинты, какие
методы, какие статус-коды, какие query-параметры ожидаются на вход, какое тело запроса, какое тело ответа будет вам
возвращено, какие ошибки вы должны обработать и так далее, версия API в том числе. В общем, всю-всю необходимую
информацию для того, чтобы вы с этой API смогли начать работать.
Про OpenAPI поговорили. Теперь поговорим про Swagger. Swagger по сути это имплементация OpenAPI. Это, скажем так,
набор инструментов для документации и визуализации нашего REST API. Выглядит это дело всё вот таким образом. В
большинстве своём на всех языках программирования, во многих фреймворках есть инструментарий, который позволяет
автоматически, почти без каких-либо дополнительных действий, генерить на основе ваших эндпоинтов всю эту
документацию. Вы можете её только как-то дополнять.
И на этом про REST всё. Перед тем как переходить к SOAP, предлагаю подвести итоги. Итак, модель взаимодействия по
REST - это клиент-сервер. Система может быть многоуровневой или многослойной - называйте как хотите. Также REST не
должно обладать каким-то состоянием - каждый раз клиент и сервер общаются как в первый раз. Должно обладать
единообразным унифицированным интерфейсом (про это мы говорили более подробно). Также API может кэшироваться. Обмен
данными чаще всего JSON. API должна быть версионирована. В ней не должно происходить... Ну и в идеале, конечно,
ваша API должна быть задокументирована. И на этом с REST мы заканчиваем и переходим к SOAP.
Как мы выяснили, REST - это не протокол, это архитектурный стиль, набор правил. В свою очередь, SOAP - это уже
протокол обмена структурированными сообщениями. Если в REST у нас может быть любой формат данных - JSON, XML либо
какой-то ещё, то в SOAP это XML. Если быть точнее - SOAP XML со своей спецификой. При этом, как я рассказывал в
своих архитектурных скринкастах, никто не запрещает вам в рамках одного приложения, в рамках одного сервера, в
рамках одного бэкенда реализовать и REST, и SOAP. То есть у вас бизнес-логика - это какое-то отдельное ядро, есть
REST-контроллеры, которые возвращают JSON, есть SOAP-контроллеры, в которых описан WSDL и которые возвращают XML.
Если REST - это набор правил по эффективному использованию HTTP, то SOAP в свою очередь может использоваться с
любым протоколом прикладного уровня: SMTP, FTP, HTTP и другие. Для описания SOAP-сервисов используется WSDL. О нём
я чуть ранее уже упоминал - это определённый язык, который основан на XML. И выглядит это всё примерно вот таким
вот образом. Здесь нет эндпоинтов в классическом понимании как в REST, здесь есть так называемые операции.
Или множество дверей, каждую из которых мы можем открыть. В свою очередь, SOAP - это скорее одно окно, в которое
нам необходимо передать название процедуры, название операции, которую мы хотим выполнить. SOAP в отличие от REST
обладает определённой строгостью, потому что REST - это всё-таки какой-то набор правил, архитектурный стиль, а SOAP
- это уже протокол, который задаёт определённые рамки, определённые границы. И сообщения, которые отправляются в
SOAP, обладают определённой структурой. Состоят они из четырёх частей: три обязательных и одна опциональная (она
отвечает за ошибки). На слайде как раз отображены три обязательные части. Предлагаю по каждой кратенько пройтись и
разобрать, для чего это нужно.
Envelope - это корневой элемент, который определяет начало и конец сообщения. Именно благодаря нему клиент
понимает, когда сообщение полностью получено. Header - это что-то типа заголовков в обычном HTTP-запросе. Даёт нам
возможность определять какие-то дополнительные свойства для приложения. Например, опять же, отправить какой-то
токен, указать тип формата сообщения и любую другую вспомогательную информацию. Ну и третий обязательный элемент
сообщения - это Body, в котором мы передаём уже какую-то полезную нагрузку, какие-то полезные данные.
Если кратко пробежаться по SOAP, то это всё, что можно сказать. Кому интересно - можете изучить уже более подробно.
Но если кратко подытожить, то опять же REST - это просто набор рекомендаций в целом со свободной структурой, и SOAP
в свою очередь - это уже протокол, который определяет строгие правила, строгую структуру и загоняет нас в
определённые границы. Здесь описываются сервисы с помощью WSDL, и сообщения обмена сообщениями происходят в формате
XML. Часто SOAP используется, как я говорил ранее уже, в банковской сфере. На этом SOAP мы заканчиваем и переходим
к GraphQL.
GraphQL - язык запросов. По GraphQL у меня на канале есть отдельный практический ролик, рекомендую к просмотру. Но
сейчас мы разберём теоретическую часть. Начнём с того, что GraphQL - это уже не архитектурный стиль и не протокол.
GraphQL - это язык запросов. И предлагаю для лучшего понимания начать с проблематики.
Представим, что у нас магазин электроники (который мы кстати разрабатывали в одном из видео на моём канале), и
здесь есть страница детального просмотра информации по конкретному товару. Здесь запрашивается много данных: всякие
характеристики, описания, стоимость - в общем много-много всего по конкретному товару. Также у нас есть другая
страница, где мы отображаем товары уже списком, и здесь нам необходимо запросить мало данных. То есть нам
достаточно отобразить модель товара, там его рейтинг и, например, стоимость. Можем ещё немного пофантазировать и
представить, что у нас есть третья страница, например, какая-нибудь админка, где нам нужно отобразить данные в виде
таблицы какой-нибудь для админов. И там нам нужно уже больше данных.
Так вот, при классическом подходе нам бы пришлось делать следующее: либо у нас был бы один endpoint, который
возвращал бы массив абсолютно всех данных, которые нам нужны (то есть объект со всеми вложенными полями, которые
нам даже могут быть в конкретном запросе не нужны), либо же нам на бэкенде бы делать разные эндпоинты. Вот здесь
вот я в примере привёл "Little Data", "Big Data". Ну конечно так никто не называет, это просто для примера. То есть
в одном запросе мы возвращаем нужные данные для одной страницы, в другом запросе - нужные данные для третьей
страницы, для четвёртой и так далее. То есть это не оптимальный подход, и хотелось бы иметь возможность, чтобы
клиент сам определял, какие данные ему в конкретном месте нужны.
То есть схематично мы хотим иметь какую-то такую картину: клиент отправляет запрос за продуктами, за товарами и
говорит: "Дай мне данные с идентификатором, названием и стоимостью", и на сервере все лишние данные, характеристики
отбрасываются, и возвращаются товары только с теми полями, которые мы запросили. И вот GraphQL как раз для этого и
предназначен.
Если при классическом подходе сервер определяет схему и формат данных, которые возвращаются в конкретном эндпоинте,
то в GraphQL сервер как раз определяет только схему данных, а клиент уже сам запрашивает те данные, те поля, те
характеристики, те атрибуты, которые ему требуются. В одном запросе нужно три поля из объекта - запрашиваем три. В
другом нужно 23 - значит, можем запросить все 23 поля.
В GraphQL есть два основных вида запроса: это Query и Mutation. Query - это аналог GET-запроса. Mutation - это
аналог POST-запроса. То есть с помощью Query мы какие-то данные получаем, с помощью Mutation либо создаём, либо
меняем. Также есть ещё Subscription - это realtime-какие-то изменения, своего рода подписка на изменения данных.
Подписки сделаны поверх WebSockets, о которых мы позже будем также говорить.
Теперь давайте посмотрим, как с этим всем работать. В первую очередь описывается схема данных. В принципе, это
похоже на обычные типы, интерфейсы, где просто описываются поля и тип этих полей. После того как мы описали схему,
сделали какую-то логику на бэкенде, с фронта мы можем запрашивать данные вот таким вот образом. В данном случае у
Hero мы запрашиваем только одно поле - name. Справа вы можете видеть то, что вернул нам эндпоинт. Также мы можем
запрашивать какие-то вложенные данные. Например, у каждого героя есть массив friends, и также вложенные какие-то
поля - только те, что нам нужны, мы можем запросить.
На вход в запрос можно передавать какой-то Input - это аргументы. Например, мы можем указать, что мы хотим получить
человека с id = 1000 и хотим запросить name и height. При этом какие-то часто запрашиваемые поля мы можем вынести в
так называемые Fragments и использовать их в разных запросах. Ну здесь, думаю, всё понятно. Это сделано для того,
чтобы мы могли какие-то части не описывать каждый раз заново, а переиспользовать в одном месте, в другом месте, в
третьем месте. С Query и с Fragments, надеюсь, всё понятно.
Теперь давайте поговорим о Mutation. Mutation - это по сути тоже обыкновенный запрос, но который как-то данные
мутирует: изменяет, создаёт что-то, выполняет какое-то действие, обновляет какую-то сущность и так далее. На слайде
вы также можете увидеть пример: есть какой-то Input на входе, указываем поля, которые мы хотим получить в качестве
тела ответа, и соответственно отправляем запрос. Эндпоинт нам возвращает только нужные данные. Работает это по сути
точно так же: запрашиваем нужные данные, отправляем Mutation, сервер нам нужные данные возвращает.
Перед тем как продолжить, предлагаю взглянуть на пример проекта. Вот таким вот образом описывается схема: есть у
нас User, у которого есть список каких-то постов, есть отдельно описанный тип Post. Описываются Inputs - это то,
что мы ожидаем на вход. Описываются соответственно все типы. Описываются Queries. Описываются Mutations.
Описывается то, что мы ожидаем на вход и то, что возвращают все эти Queries или эти Mutations. То есть getAllUsers
возвращает массив пользователей, getUser возвращает по id конкретного пользователя, и mutation createUser с таким
вот Input создаёт нам пользователя.
Как видите, здесь в принципе максимально просто, интуитивно понятно. Далее описываются сами эндпоинты,
бизнес-логика какая-то. Здесь мы по id пользователя находим, здесь возвращаем всех пользователей, а здесь
пользователя создаём, добавляем его в массив, там присваиваем какой-то идентификатор, с базой данных
взаимодействуем. Потом указываем URL, по которому будет всё это дело доступно, указываем схему и запускаем сервер.
И далее по вот такому вот пути, который мы указали вот здесь вот (как видите, graphql - это вот это значение,
которое мы передали вот здесь) нам будет доступна вот такая своего рода админка. Здесь мы можем посмотреть все
запросы, все мутации, посмотреть какого типа у нас данные. И что удобно - это всё мы отдаём фронту, и фронт сам
определяет, какие данные ему нужны. Соответственно, он может запрашивать только то, что требуется ему в конкретной
ситуации.
Также всё можем посмотреть по мутациям, поискам - то, что нам необходимо найти. И в общем достаточно удобно этим
всем пользоваться. Здесь же мы, кстати, можем попробовать написать прямо запрос. Давайте это и сделаем. Попробуем
написать какой-нибудь запрос. При этом что важно - в одном запросе мы можем запрашивать разные данные. То есть мы
можем делать такие комбинированные запросы. Указываем запрос за получением списка всех пользователей, и мы хотим от
бэкенда получить id, username. Ну и давайте ещё age. Отправляем запрос - видим, что нам вернул эндпоинт. Пробуем
запросить только id - видим, что он вернул нам только id. Попробуем запросить ещё какую-нибудь вложенную сущность,
например dishes и title. Отправляем запрос. Ну да, посты он вернул нам, потому что я их просто там не указал, но
если бы они были, то он бы вернул нам посты только с указанными полями.
Ну и теперь обсудим преимущества, которые GraphQL нам предоставляет. Во-первых, это частичная
самодокументируемость. Все Inputs, все типы - это всё частично самодокументируется. Схема - это кодогенерируемая.
Опять же за счёт строгой схемы мы можем генерировать в том числе и на фронтенде код. Флоу работы здесь следующий:
мы описываем какой-то запрос, например, получить список всех todo-шек с такими-то полями. Далее запускаем
определённый скриптик с помощью определённого инструмента, указываем путь до схемы. После чего, во-первых, в
базовом варианте нам нагенерируют автоматически все типы. Нам не придётся описывать их вручную, и у нас будет ну
практически полное соответствие типов на фронтенде и на бэкенде. Но также это дело всё можно ещё по-тюнинговать. В
общем, это тоже достаточно классная фича GraphQL.
Следующее преимущество - это ну, наверное, оно самое основное - это то, что клиент запрашивает только нужные ему
данные. Из этого вытекает как раз то, что меньше трафика гоняется по сети. Мы не запрашиваем огромный набор данных,
мы запросили только то, что нам нужно в конкретной ситуации. Это влияет, естественно, и на скорость работы, и на
трафик, который потребляется клиентом. В общем, сплошные плюсы.
В общем, подводя итоги, GraphQL - классная штука, всем как минимум рекомендую попробовать. И напоминаю опять же,
что у меня есть практический ролик на канале с созданием приложения как серверного, так и клиентского.Websockets -
real time
Клиентского на graphql и на этом с graphql мы заканчиваем и переходим к веб сокетам. Вебсокеты - это также
протокол, но в отличие от http, в котором мы установили связь, отправили какой-то запрос, получили ответ и связь
оборвали, здесь устанавливается постоянное подключение. В http, конечно, тоже бывают исключения, о которых я также
на своём канале в практическом ролике по realtime приложениям рассказывал, но в классическом варианте всё-таки на
каждый запрос устанавливается новое соединение в http.
Websocket - это как раз протокол для realtime взаимодействия, когда клиент и сервер за счёт постоянного соединения
непрерывно обмениваются какими-то данными. Когда нужны вебсокеты? Это, конечно же, какие-нибудь чаты, где вы
непрерывно обмениваетесь сообщениями, это какие-нибудь графики, где нужно быстро показывать изменения, например,
там курсы валют или на бирже, когда цены на акции меняются постоянно в realtime.
Давайте наглядно посмотрим, как это работает. Начнём с http: похожая схема - у нас есть какой-то сервер, который с
базой данных общается, здесь мы отправляем запрос, получаем ответ, и на этом соединение как бы обрывается. При
этом, если подразумевается какая-то широковещательная рассылка, например, отправка сообщения в чат, то для того,
чтобы остальные пользователи получили это сообщение, они должны его запросить, то есть отправить повторный запрос.
В вебсокетах это работает немного по-другому: все, грубо говоря, онлайн-пользователи устанавливают соединение с
сервером - непрерывное соединение, в котором и клиент, и сервер постоянно обмениваются какими-то сообщениями. И уже
в данном случае, если один пользователь отправляет сообщение на сервер, то сервер со всеми, с кем у него
установлено подключение, может сразу этим сообщением поделиться (как я уже говорил ранее - так называемая
широковещательная рассылка). И за счёт того, что нам не надо постоянно устанавливать соединение и обрывать это
соединение, клиент с сервером могут в таком непрерывном формате (я это наверное уже четвёртый или пятый раз говорю,
однако это важно) обмениваться сообщениями вот в таком двустороннем формате.
У меня на канале есть ролик, где мы создаём три realtime-чата на разных технологиях: это вебсокеты, это long
polling и Server-Sent Events. Ролик длится буквально полчаса, однако пользы можно извлечь очень много, если ранее
вы с этим не работали. Также по вебсокетам у меня есть отдельный ролик, где мы создаём Paint online - это такой,
грубо говоря, холст, на котором сразу несколько человек что-то рисуют и сразу видят изменения, которые на этот
холст вносятся разными участниками.
Сейчас давайте кратенько глянем на то, как вебсокеты вообще работают. Это вот сервер: простейший websocket-сервер,
вешаем какой-то handler на подключение и на сообщение (это события), и соответственно эти сообщения мы как-то
обрабатываем. Отправляем с клиента вот такое поле event, по которому мы определяем, какое действие должно
выполниться: подключение или же отправка сообщения. Также у нас есть вот такая широковещательная рассылка, где мы
по всем онлайн-клиентам проходимся в цикле и отправляем им сообщение. То есть со стороны сервера здесь всё
элементарно просто.
Давайте посмотрим теперь на клиент. Здесь в принципе тоже всё просто: устанавливаем соединение (обратите внимание,
что вместо http указываем ws - это протокол), указываем порт и, как обычно, подключаемся. Только в отличие от http
(опять же) мы не на каждом запросе это указываем, только при подключении. И затем вот в таком, опять же, событийном
формате мы с этим взаимодействуем: есть событие onOpen, onMessage, onClose и onError. Соответственно, каждое из
этих событий мы можем обработать и написать на это какую-то логику: отправка сообщения, установка подключения,
закрытие подключения. Опять же, здесь мы так теоретически кратко смотрим. Если хотите подробнее - ссылочки будут в
описании, это ролики с канала.
Мы двигаемся дальше и теперь поговорим про удалённый вызов процедур (RPC), в частности поговорим про gRPC и про
tRPC. Прежде чем говорить про конкретные имплементации, давайте вообще разберём понятие RPC. Как я уже сказал, это
удалённый вызов процедур. И что это значит? Вот у нас есть клиент и есть сервер. Сервер реализует какой-то метод,
например, doSomething или, там, я не знаю, "добавить в корзину" или ещё какое-то действие. При этом на клиенте этот
метод не реализован, но клиент с помощью специального объекта (так называемого стаба) может этот метод вызвать, и
при этом он это делает так, как будто бы этот метод реализован у него. То есть это и есть удалённый вызов
процедуры: на самом деле там отправляется сетевой запрос, сервер отправляет какие-то данные, но с точки зрения
клиента это всё происходит так, как будто бы клиент сам вызвал эту процедуру и сам выполнил какое-то действие. На
самом деле происходит именно вот такое удалённое взаимодействие.
Давайте ещё раз более схематично посмотрим: сервер имплементирует какой-то метод (с точки зрения RPC это называется
процедура), и клиент эти процедуры удалённо может вызвать. При этом с точки зрения его кода это выглядит как stub
названиеПроцедуры(). Название метода. Согласитесь, звучит достаточно удобно.
Давайте теперь поговорим про gRPC. gRPC - это как раз фреймворк (набор инструментов, платформа - можно по-разному
назвать) от Google, и внутри себя она использует как раз вот эту технологию удалённого вызова процедур. gRPC очень
популярен в микросервисной архитектуре, и вот именно микросервисы друг с другом по gRPC общаются. Чуть позже мы
поймём почему, когда будем говорить о преимуществах. Ну и, например, во всей вот этой схеме микросервисы между
собой общаются по gRPC, есть какой-то proxy/gateway, какая-то входная точка, которая может уже всё это по HTTP
отдавать куда-то в веб, в мобилку и так далее, но сами вот эти микросервисы они общаются по gRPC, делают это супер
эффективно и супер быстро за счёт преимуществ HTTP/2 и других преимуществ, о которых мы позже поговорим.
Ну и давайте сразу, недалеко отходя от кассы, о преимуществах сразу и поговорим. Поговорим о том, что отличает gRPC
от классического подхода. Во-первых, здесь используется HTTP/2 вместо HTTP/1. HTTP/2 по тестам быстрее на 10-15%,
чем HTTP/1. Вместо текстовых сообщений, которые используются в HTTP/1, в HTTP/2 используется бинарный формат, за
счёт этого его можно лучше сжать, лучше обработать, быстрее отправить. В HTTP/2 также реализованы потоки данных
(так называемый стриминг), он простой и супербыстрый. Также здесь вместо JSON, который достаточно избыточный и не
сжимается, используется бинарный формат (как я говорил уже ранее), он называется protobuf, на него мы чуть позже
тоже взглянем и поговорим о преимуществах.
Также в gRPC есть инструментарий из коробки: это генерация кода для многих языков программирования. То есть вы
описываете определённый файлик, указываете, что на входе, что на выходе, какие процедуры у вас есть внутри сервиса,
и с помощью специального генератора (с помощью специального компилятора, который называется protoc) вы можете
сгенерировать код для разных языков программирования. Это тоже делает gRPC таким более гибким, более универсальным
инструментом. Ну и, соответственно, как написано на слайде, инструментарий из коробки: там есть аутентификация,
потоковая передача данных (в том числе двунаправленная) и много-много другого. Ну и, соответственно, всё это вот,
сам подход с удалённым вызовом процедур, позволяет нам удобно эти процедуры вызывать. То есть клиент вызывает
процедуры так, как будто бы они реализованы прямо у него.
Теперь давайте взглянем на формат обмена данными, на тот самый protobuf, о котором мы говорили ранее. Это простой
бинарный формат, за счёт того, что он бинарный, его можно сжать, причём достаточно эффективно, в отличие от того же
JSON. И он имеет строгую типизацию в отличие тоже от того же JSON, в котором ты в одно поле можешь в массиве,
например, в объектах, в одном случае передать строку, в другом - number, в третьем вообще массив, и строгой
типизации это всё дела не имеет. Но есть нюанс с тем, что сервер перед тем, как отправить данные, должен их
сериализовать вот в этот вот формат protobuf, а клиент должен эти данные уже десериализовать. Но за счёт того, что
у нас нет такой избыточности, как в JSON, плюс JSON нельзя сжимать эффективно, а мы получаем большой выигрыш в
скорости, очень большой.
Теперь давайте посмотрим на то, как описываются файлики самими сервисами. Выглядит это примерно вот так: здесь у
нас описан сервис с двумя процедурами, у которых на вход мы ожидаем HelloRequest, а на выходе HelloReply с такими
вот полями. После того, как мы описали этот самый proto-файл, с помощью компилятора protoc мы можем сгенерировать
код на многих языках программирования. Что делает этот инструмент также универсальным: пишем один proto-файл,
генерим код на разных языках. Опять же, для микросервисов да и в принципе для разработки супер удобно.
Я выкачал с официальной документации проект notes, давайте на него взглянем. Вот тут вот лежат proto-файлы,
процедуры описывают, что они получают на входе и что отдают на выходе. То есть здесь пример классического Hello
world в обычном формате и в стриминговом. Затем какие-то библиотеки, что-то настраивается, далее описывается
sayHello - это соответственно имплементация самой процедуры, здесь вызывается просто callback и передаётся
сообщение "Hello, " + name, который мы получили в request. Вот это тот самый request, который мы описывали в
proto-файле. Прямо примитивный классический Hello world, но нам для базового понимания хватит.
Далее вот этот вот сервис, который мы в proto-файле описали, подключается, указываются процедуры (методы), далее
сервер запускается. И теперь давайте посмотрим, как клиент с этим всем работает. Опять же, настраиваются какие-то
библиотеки, ещё что-то - нас это особо не интересует, но нас интересует вот этот момент: мы создаём клиент из
сервиса Greeter, и теперь мы можем удалённо процедуры вызывать. То есть, обратите внимание, мы вызываем процедуру
так, как будто бы мы вызываем её у этого клиента: client.sayHello(). Мы не отправляем каких-то отдельных запросов,
мы не описываем какие-то там HTTP-заголовки, методы, мы просто вызываем процедуру и передаём в неё аргументы. В
нашем случае это request-объект с полем name и callback, в котором мы просто выводим в логи сообщение, которое нам
сервер как раз вернул.
Давайте теперь запустим сервер, запустим клиент, и мы видим вот это самое сообщение. То есть процедура была вызвана
удалённо, мы получили какой-то message и в логе его вывели. Если вот тут username поменяем, соответственно видим,
что нам сервер возвращает уже другое значение. Соответственно, в процедуре может быть любая логика. Сейчас мы по
сути просто вызываем callback и передаём туда строчку сам message, но здесь может быть что-то посложнее.
Давайте теперь руками попробуем ещё одну процедуру добавить, посмотрим, как это происходит. Добавляем сюда rpc
sayGoodbye, дублируем request, меняем у него название будет GoodbyeRequest, соответственно поля можно тоже какие-то
другие добавить. Здесь у нас в принципе всё. После того как вы описали proto-файл, с помощью protoc можно
сгенерировать статический код. Ну давайте сейчас более быстрым путём пойдём, нам здесь в принципе один метод
добавить, чтобы просто убедиться, что это заработает. Вот и давай теперь на клиенте этот метод вызовем: sayGoodbye,
в логи передаём сообщение, которое нам возвращает сервер. Ну и давайте запустим: запускаем сервер и запускаем
клиент. И как видите, мы получаем два сообщения: "Hello, Вася", "sayGoodbye, Вася". Надеюсь, какое-то базовое
понимание gRPC у вас сложилось, чтобы вы примерно представляли, что это такое, и дальше могли уже изучить
самостоятельно. Возможно, когда-то я более подробный ролик про gRPC практически запишу.
Напоследок посмотрим на tRPC - относительно новый инструмент, расшифровывается как typesafe RPC. В принципе, здесь
наверное какую-то теорию давать смысла нет, давайте посмотрим лучше сразу на пример. Также выкачал пример с
официальной документации, здесь куча всяких примеров, и давайте посмотрим минимальный с использованием React.
Начнём с сервера: опять же, сверху все необходимые импорты, далее инициализируется сам tRPC, и нас интересует по
большей части router. Здесь описывается вот в таком формате то, что мы ожидаем на входе (сам input и ctx - это
непосредственно сам запрос), какая-то логика. Соответственно, это всё можно вот так вот вынести отдельно, но
понятное дело, здесь сейчас оставим всё вот в таком виде. И самое важное, что мы здесь описываем схему того, какой
объект мы ожидаем на входе, какой объект мы можем ожидать на выходе. Давайте добавим здесь ещё какое-нибудь поле.
Здесь вот, как видите, много вариантов того, какой тип мы можем указать: z.string() и так далее, можно указать
какие-то модификаторы, что поле может быть пустым, что поле может принимать такое значение, такое значение и так
далее.
Соответственно, здесь указывается тип этого input'а, создаётся HTTP-сервер классический, настраивается CORS и так
далее. Здесь всё в принципе по классике, как в обычном каком-нибудь Express. Но что самое интересное - это то, что
на клиенте мы эти процедуры все можем удалённо как бы вызывать, и при этом вся типизация сохранена. Вот здесь,
обратите внимание, используется React Query, настраивается вот этот tRPC-клиент через Provider. В принципе, всё по
классике, классический React. В папочку utils давайте ещё заглянем: вот здесь создаётся tRPC React-клиент, и там
указывается вот этот самый router, который мы с сервера импортировали. Мы просто вызываем какие-то необходимые для
нас хуки. Но что самое интересное, что вся типизация сохранена. Как видите, автокомплит нам сразу подсказывает, что
у нас ожидается на вход поле name, которое является строкой, и поле value, которое является массивом. При этом за
счёт вот этой строгой типизации мы также сразу видим, какой набор методов, какой набор процедур у нас есть, и эти
процедуры мы можем вызывать. Сейчас у нас показывает, что массив на вход ожидается типа any, но это наверняка тоже
как-то можно типизировать.
Не знаю, насколько это production-ready технология, насколько она вообще хороша в продакшене, но для каких-то вот
таких небольших проектов думаю, клепать можно очень быстро. Выглядит всё супер круто, особенно вот в связке с тем
же React Query выглядит классно. Давайте попробуем какой-нибудь ещё метод, какую-нибудь процедуру добавить,
посмотрим, как всё это дело у нас подхватит на клиенте. Ну да, сразу видим, что у нас появился метод goodbye, можем
вызвать useQuery. Ну и соответственно, вся типизация работает. Если мы какое-нибудь поле уберём, то оно сразу же
пропадает. 